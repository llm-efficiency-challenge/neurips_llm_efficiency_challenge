{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this notebook to generate texts using Falcon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ixZpWQf-MFZ",
    "outputId": "52be5ded-0640-4510-b605-13672f35893a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'lit-gpt'...\n",
      "remote: Enumerating objects: 1527, done.\u001B[K\n",
      "remote: Counting objects: 100% (375/375), done.\u001B[K\n",
      "remote: Compressing objects: 100% (125/125), done.\u001B[K\n",
      "remote: Total 1527 (delta 310), reused 271 (delta 249), pack-reused 1152\u001B[K\n",
      "Receiving objects: 100% (1527/1527), 495.45 KiB | 5.38 MiB/s, done.\n",
      "Resolving deltas: 100% (996/996), done.\n"
     ]
    }
   ],
   "source": [
    "# clone Lit-GPT\n",
    "!git clone https://github.com/Lightning-AI/lit-gpt\n",
    "%cd lit-gpt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJgQZwzi-UQ2"
   },
   "outputs": [],
   "source": [
    "# for CUDA\n",
    "!pip install --index-url https://download.pytorch.org/whl/nightly/cu118 --pre 'torch>=2.1.0dev' -q\n",
    "\n",
    "# install the dependencies\n",
    "!pip install huggingface_hub -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JVnohWmg-afB"
   },
   "outputs": [],
   "source": [
    "# download the weights\n",
    "!python scripts/download.py --repo_id tiiuae/falcon-7b\n",
    "!python scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/tiiuae/falcon-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pfF2Il6N-nM1",
    "outputId": "486d0c66-b071-4c48-8590-55ffe565d0e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load model: 148.99 seconds.\n",
      "Global seed set to 1234\n",
      "Hello, my name is Jack.\n",
      "Some people think that dogs are like people. They see them as loyal, loving, and caring friends. I don't believe that's true. Dogs are animals. Animals are animals. I'm not a dog person.\n",
      "Time for inference 1: 7.82 sec total, 6.39 tokens/sec\n",
      "Memory used: 10.11 GB\n"
     ]
    }
   ],
   "source": [
    "# run inference\n",
    "!python generate/base.py \\\n",
    "        --prompt \"Hello, my name is\" \\\n",
    "        --checkpoint_dir checkpoints/tiiuae/falcon-7b \\\n",
    "        --quantize bnb.int8"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
